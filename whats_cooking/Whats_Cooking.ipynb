{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle's What's Cooking competition\n",
    "https://www.kaggle.com/c/whats-cooking\n",
    "\n",
    "Below I present the code which allowed me to jump to the 5th position on the leaderboard. I was experimenting with various feature engeneering and with dosens of classifiers - the final submission is just a single [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). The key idea was to utilize the relations between ingredients - they boosted the power of the classifier a lot!\n",
    "\n",
    "### Competition\n",
    "The competions was about predicting the type of cuisine of a dish based on its ingredients. The training set constisted of 39774 recipies, looking like this (I omitted id):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sample = {\n",
    "    'cuisine': 'greek',\n",
    "    'ingredients': [\n",
    "        'romaine lettuce',\n",
    "        'black olives',\n",
    "        'grape tomatoes',\n",
    "        'garlic',\n",
    "        'pepper',\n",
    "        'purple onion',\n",
    "        'seasoning',\n",
    "        'garbanzo beans',\n",
    "        'feta cheese crumbles'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task was to correctly predict cuisines of other 9944 recipes. There were 20 cuisines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipes:  39774\n",
      "cuisines ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican', 'japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('data/train.json')\n",
    "print('recipes: ', data.shape[0])\n",
    "print('cuisines', sorted(set(data.cuisine)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-ingredients\n",
    "\n",
    "In order to build a classifier we have to somehow translate ingredients into values. A common approach is to use the bag-of-words model. First all words from recipes are retrieved to build a vocabulary - a list of unique words. Given the indices of words in the list, the recipes are converted into vectors, where each entry represent the number of appearences of an ingredient in the recipe. I use the sklearn implementation i.e. [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>carrot</th>\n",
       "      <th>cucumber</th>\n",
       "      <th>pepper</th>\n",
       "      <th>salt</th>\n",
       "      <th>sugar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  carrot  cucumber  pepper  salt  sugar\n",
       "0      1       0         0       1     1      1\n",
       "1      0       1         1       0     1      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "recipes = [\n",
    "    'salt, sugar, black pepper',\n",
    "    'cucumber, carrot, salt'\n",
    "]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vectors = vect.fit_transform(recipes).todense()\n",
    "\n",
    "pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the ingredients are already in a list form, so we have to slightly modify the *tokenizer* method of CountVectorizer, namely we'll join all the ingredients into a single string. Also, as we shouldn't be differentiating between various forms of the same word, e.g. 'egg' - 'eggs', 'fry' - 'fried', we'll perform stemming. Below is presented a helper class **StemmerTokenizer** dealing with the above issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class StemmerTokenizer(object):\n",
    "    \"\"\"\n",
    "    Joins all ingredients into a single string and provides\n",
    "    a list of stems of all words longer than 2 letters.\n",
    "    \n",
    "    Example:\n",
    "    >>> tok = StemmerTokenizer()\n",
    "    >>> tok.tokenizer(\n",
    "            tok.preprocessor([\n",
    "                'romaine lettuce', 'black olives', 'grape tomatoes',\n",
    "                'garlic', 'pepper', 'purple onion', 'seasoning',\n",
    "                'garbanzo beans', 'feta cheese crumbles'\n",
    "            ])\n",
    "        )\n",
    "    ['romain', 'lettuc', 'black', 'oliv', 'grape', 'tomato',\n",
    "     'garlic', 'pepper', 'purpl', 'onion', 'season',\n",
    "     'garbanzo', 'bean', 'feta', 'chees', 'crumbl']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b'\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "\n",
    "    def mapper(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "\n",
    "    def tokenizer(self, doc):\n",
    "        return [self.mapper(t) for t in regexp_tokenize(doc, pattern=self.pattern)]\n",
    "\n",
    "    def preprocessor(self, line):\n",
    "        return ' '.join(line).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is an example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black  cheddar  chees  chili  extra  fresh  garlic  ground  \\\n",
       "0      1      1        0      0      0      1      1       1       0   \n",
       "1      0      1        1      1      1      0      0       0       1   \n",
       "\n",
       "   jalapeno  lime  oil  oliv  onion  pepper  pork  salt  tomato  virgin  \n",
       "0         0     0    1     1      0       1     0     1       1       1  \n",
       "1         1     1    1     1      1       1     1     1       0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = [\n",
    "    ['tomatoes', 'fresh basil', 'garlic', 'extra-virgin olive oil', 'salt', 'black pepper'],\n",
    "    ['olive oil', 'onion', 'pork', 'cheddar cheese', 'ground black pepper', 'salt', 'lime', 'jalapeno chilies'],\n",
    "]\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    preprocessor=StemmerTokenizer().preprocessor,\n",
    "    tokenizer=StemmerTokenizer().tokenizer)\n",
    "vectors = vect.fit_transform(recipes)\n",
    "\n",
    "pd.DataFrame(data=vectors.todense(), columns=sorted(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the ingredients one could guess that the first recipe is probably italian or french - we can pick typical products, eg.: as extra-vergin olive oil, fresh basil. The second one is probably mexican - jalapeno chillies are good indicators. So, the first thing we do to recognize the cuisine is to filter out the most common ingredients such as salt, black pepper, olive oil and focus on the most unique ones. It would be of help to incorporate the information about 'commonness' into the above vectors.\n",
    "\n",
    "The answer is: term frequency–inverse document frequency or [tf-idf](https://en.wikipedia.org/wiki/Tf-idf). I'm not going to go into details, it is a common approach when working with bags-of-words, we will rather focus on outcomes. Again, we use the scikit-learn implementation [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). We use variables from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black cheddar  chees  chili  extra  fresh garlic ground jalapeno  \\\n",
       "0  0.342  0.244   0.000  0.000  0.000  0.342  0.342  0.342  0.000    0.000   \n",
       "1  0.000  0.219   0.308  0.308  0.308  0.000  0.000  0.000  0.308    0.308   \n",
       "\n",
       "    lime    oil   oliv  onion pepper   pork   salt tomato virgin  \n",
       "0  0.000  0.244  0.244  0.000  0.244  0.000  0.244  0.342  0.342  \n",
       "1  0.308  0.219  0.219  0.308  0.219  0.308  0.219  0.000  0.000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "trans = TfidfTransformer()\n",
    "tfidf = trans.fit_transform(vectors)\n",
    "\n",
    "pd.DataFrame(data=tfidf.todense(), columns=sorted(vect.vocabulary_)).applymap(lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that ingredients appearing in single recipe have larger weights than the common ones - that's what we wanted! To use CountVectorizer and TfidfTransformer easier, we will chain them in a [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) (we could be using a [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but we keep them separatelly so it's clearer what's going on). We'll reproduce now the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black cheddar  chees  chili  extra  fresh garlic ground jalapeno  \\\n",
       "0  0.342  0.244   0.000  0.000  0.000  0.342  0.342  0.342  0.000    0.000   \n",
       "1  0.000  0.219   0.308  0.308  0.308  0.000  0.000  0.000  0.308    0.308   \n",
       "\n",
       "    lime    oil   oliv  onion pepper   pork   salt tomato virgin  \n",
       "0  0.000  0.244  0.244  0.000  0.244  0.000  0.244  0.342  0.342  \n",
       "1  0.308  0.219  0.219  0.308  0.219  0.308  0.219  0.000  0.000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(\n",
    "            preprocessor=StemmerTokenizer().preprocessor,\n",
    "            tokenizer=StemmerTokenizer().tokenizer)),\n",
    "        ('transformer', TfidfTransformer()),\n",
    "    ])\n",
    "\n",
    "vectors = pipeline.fit_transform(recipes)\n",
    "pd.DataFrame(data=vectors.todense(),\n",
    "             columns=sorted(pipeline.named_steps['vectorizer'].vocabulary_)\n",
    "            ).applymap(lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations between words\n",
    "\n",
    "My first classifiers were based on the above vectorization of recipes. However, I shortly realized that listing all words without preserving the relations between them brings significant lose of information. Simple example, if we vectorize these two ingredients:\n",
    "\n",
    " - 'red pepper, black olives'   -> ['black', 'olive', 'pepper', 'red']\n",
    " - 'black pepper, green olives' -> ['black', 'olive', 'pepper', 'green']\n",
    "  \n",
    "The only difference now is 'red' and 'green', we cannot discriminate anymore between 'red pepper' and 'black pepper'. This is not a minor case, there's plenty of such overlaps.\n",
    "\n",
    "However, this is not the only argument to introduce some means of preserving the relation between words. We will see it by examining recipes with ingredients often present in certain cuisines, but not unique for them. Let's check amount of **dijon** and **wine** in the french cusine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french</th>\n",
       "      <th>non-french</th>\n",
       "      <th>fraction in cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dijon</th>\n",
       "      <td>190</td>\n",
       "      <td>380</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>613</td>\n",
       "      <td>3551</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dijon + wine</th>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              french  non-french fraction in cuisine\n",
       "dijon            190         380                0.33\n",
       "wine             613        3551                0.15\n",
       "dijon + wine      87          86                0.50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recipes_with(ingrs, df):\n",
    "    df = df.copy()\n",
    "    if ~isinstance(ingrs, list):\n",
    "        ingrs = [ingrs]\n",
    "    for ingr in ingrs:\n",
    "        df = df[df.ingredients.apply(lambda row: ingr in row)]\n",
    "    return df\n",
    "  \n",
    "def recipes_of(cuisine, df):\n",
    "    df_in = df[df.cuisine == cuisine].copy()\n",
    "    df_out = df[df.cuisine != cuisine].copy()\n",
    "    return df_in, df_out\n",
    "\n",
    "def fraction(df0, df1):\n",
    "    return '%.2f' % (len(df0) / (len(df0) + len(df1)))\n",
    "\n",
    "ingrs = ('dijon', 'wine')\n",
    "cuisine = 'french'\n",
    "\n",
    "data = pd.read_json('data/train.json')\n",
    "data['ingredients'] = data.ingredients.apply(lambda ingrs: ' '.join(ingrs))\n",
    "df0in, df0out = recipes_of(cuisine, recipes_with(ingrs[0], data))\n",
    "df1in, df1out = recipes_of(cuisine, recipes_with(ingrs[1], data))\n",
    "df01in = pd.merge(df0in, df1in, how='inner')\n",
    "df01out = pd.merge(df0out, df1out, how='inner')\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=[\n",
    "        [len(df0in), len(df0out), fraction(df0in, df0out)],\n",
    "        [len(df1in), len(df1out), fraction(df1in, df1out)],\n",
    "        [len(df01in), len(df01out), fraction(df01in, df01out)]],\n",
    "    index=list(ingrs) + ['%s + %s' % ingrs],\n",
    "    columns=[cuisine, 'non-' + cuisine, 'fraction in cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above table, that:\n",
    " - 33% of recipes with dijon are french\n",
    " - 15% of recipes with wine are french\n",
    "\n",
    "This is a very important information when classifying. However if we consider a simultaneous appearance of both dijon and wine in the recipe we get **50% chances the recipe is french!** This is a significant increase of our odds!\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "We should use not only single words derived from a recipe, we should also use pairs of words - it allows us to preserve some valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DupleTokenizer\n",
    "\n",
    "Below I introduce *DupleTokenizer* - class inheriting from *StemmerTokenizer*, but with additional method to create pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DupleTokenizer(StemmerTokenizer):\n",
    "    \"\"\"\n",
    "    Builds upon the StemmerTokenizer: after all words in a recipe are stemmed\n",
    "    they are grouped in all possible combinations of two words and added\n",
    "    to the words' list.\n",
    "    \n",
    "    Example (simplified):\n",
    "    >>> tok = DupleTokenizer()\n",
    "    >>> tok.tokenizer(\n",
    "            tok.preprocessor([\n",
    "                'sugar', 'salt', 'black pepper'\n",
    "            ])\n",
    "        )\n",
    "    ['black', 'pepper', 'salt', 'sugar',\n",
    "     'black pepper', 'black salt', 'black sugar', 'pepper salt', 'pepper sugar', 'salt sugar']\n",
    "    \"\"\"\n",
    "\n",
    "    def duple(self, x):\n",
    "        duples = []\n",
    "        x_len = len(x)\n",
    "        i = 0\n",
    "        while i < x_len-1:\n",
    "            j = i + 1\n",
    "            while j < x_len:\n",
    "                duples.append('%s %s' % (x[i], x[j]))\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "        return np.array(duples)\n",
    "\n",
    "    def tokenizer(self, doc):\n",
    "        words = np.array([self.mapper(t) for t in regexp_tokenize(doc, pattern=self.pattern)])\n",
    "        words = sorted(set(words))\n",
    "        words = np.hstack([words, self.duple(words)])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the above tokenizer, we get some 100 times more features:\n",
    " - StemmerTokenizer: 2598\n",
    " - DupleTokenizer: 292246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_count(tokenizer, df):\n",
    "    vect = CountVectorizer(\n",
    "        preprocessor=tokenizer().preprocessor,\n",
    "        tokenizer=tokenizer().tokenizer)\n",
    "    return len(vect.fit(df.ingredients).vocabulary_)\n",
    "\n",
    "# It takes around 40s, so I commented it out\n",
    "# data = pd.read_json('data/train.json')\n",
    "# print('single words:           ', features_count(StemmerTokenizer, data))\n",
    "# print('single words and pairs: ', features_count(DupleTokenizer, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "After extracting features we can focus on classifier. We have around 40k training samples of 300k features, so there is much more features than observations. In such a situation we can experience curse of dimensionality and there is a high risk of overfitting. One approach to deal with this problem would be to try to reduce the number of features. However, it turns out that a direct use of a SVM with a linear kernel works very well [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). This classifier, if properly regularized by choosing a proper penalty parameter, is highly resistant to over-fitting. To deal with multiple classes the model is trained in one-vs-rest regime. It is also worth to note that linearSVC is pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "    \n",
    "tokenizerClass = DupleTokenizer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        preprocessor=tokenizerClass().preprocessor,\n",
    "        tokenizer=tokenizerClass().tokenizer,\n",
    "        stop_words='english',\n",
    "        max_df=1.0,\n",
    "        min_df=1,\n",
    "        binary=True,\n",
    "    )),\n",
    "    ('transformer', TfidfTransformer()),\n",
    "    ('classifier', LinearSVC(\n",
    "        C=0.78, penalty='l2', loss='squared_hinge', dual=True, max_iter=1000, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes around 50s\n",
    "train = pd.read_json('data/train.json')\n",
    "X_train = train['ingredients']\n",
    "y_train = train['cuisine']\n",
    "\n",
    "test = pd.read_json('data/test.json')\n",
    "X_test = test['ingredients']\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "prediction = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = test.copy()\n",
    "submission['cuisine'] = prediction\n",
    "submission.to_csv(\n",
    "    'submission.csv', index=False, quoting=3,\n",
    "    columns=['id', 'cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a recipe to jump to 5th place!\n",
    "![score](score.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
