{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle's What's Cooking competition\n",
    "**20 Dec 2015**\n",
    "\n",
    "Below, I present the solution to the [Kaggle's What's Cooking competition](https://www.kaggle.com/c/whats-cooking) which allowed me to jump to the **5th** position on the leaderboard, out of 1388 competitors.\n",
    "\n",
    "During the competition time I focused mainly on feature engeneering. The final submission is just a single [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). I experimented also with other various classification models: multilayer perceptrons, factorization machines, tree-based models - among them the linear SVC yields the best results. I imagine that an ensemble of the predictions from several models would result in even better score.\n",
    "\n",
    "The key idea was to take advantage of the relations between ingredients - I describe this below. It boosted the final score a lot!\n",
    "\n",
    "### Competition\n",
    "The competition was about predicting the type of the dish's cuisine based on its ingredients. The training set consisted of 39774 recipes, which looked like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sample = {\n",
    "    'cuisine': 'greek',\n",
    "    'ingredients': [\n",
    "        'romaine lettuce',\n",
    "        'black olives',\n",
    "        'grape tomatoes',\n",
    "        'garlic',\n",
    "        'pepper',\n",
    "        'purple onion',\n",
    "        'seasoning',\n",
    "        'garbanzo beans',\n",
    "        'feta cheese crumbles'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 20 cuisines in total and the task was to correctly predict cuisines of other 9944 recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipes: 39774\n",
      "cuisines: [u'brazilian', u'british', u'cajun_creole', u'chinese', u'filipino', u'french', u'greek', u'indian', u'irish', u'italian', u'jamaican', u'japanese', u'korean', u'mexican', u'moroccan', u'russian', u'southern_us', u'spanish', u'thai', u'vietnamese']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('data/train.json')\n",
    "print 'recipes:', data.shape[0]\n",
    "print 'cuisines:', sorted(set(data.cuisine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-ingredients\n",
    "\n",
    "In order to build a classifier we have to somehow translate ingredient list into a numeric representation. A common approach is to use the bag-of-words model. First, all words from recipes are retrieved to build a vocabulary - a list of all words in the dataset. Given the indices of words in the list, the recipes are converted into vectors, where each entry represent the number of appearances of an ingredient in the recipe. I use the sklearn implementation i.e. [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>black</th>\n",
       "      <th>carrot</th>\n",
       "      <th>cucumber</th>\n",
       "      <th>pepper</th>\n",
       "      <th>salt</th>\n",
       "      <th>sugar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   black  carrot  cucumber  pepper  salt  sugar\n",
       "0      1       0         0       1     1      1\n",
       "1      0       1         1       0     1      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "recipes = [\n",
    "    'salt, sugar, black pepper',\n",
    "    'cucumber, carrot, salt'\n",
    "]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vectors = vect.fit_transform(recipes).todense()\n",
    "\n",
    "pd.DataFrame(data=vectors, columns=sorted(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the ingredients are already in a list form, so we have to slightly modify the *tokenizer* method of CountVectorizer, namely we'll join all the ingredients together into a single string. Also, as we shouldn't be differentiating between various forms of the same word, e.g. 'egg' - 'eggs', 'fry' - 'fried', we'll perform stemming. The helper class **StemmerTokenizer** dealing with the above issues is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class StemmerTokenizer(object):\n",
    "    \"\"\"\n",
    "    Joins all ingredients together into a single string and provides\n",
    "    a list of stems of all words longer than 2 letters.\n",
    "\n",
    "    Example:\n",
    "    >>> tok = StemmerTokenizer()\n",
    "    >>> tok.tokenizer(\n",
    "            tok.preprocessor([\n",
    "                'romaine lettuce', 'black olives', 'grape tomatoes',\n",
    "                'garlic', 'pepper', 'purple onion', 'seasoning',\n",
    "                'garbanzo beans', 'feta cheese crumbles'\n",
    "            ])\n",
    "        )\n",
    "    ['romain', 'lettuc', 'black', 'oliv', 'grape', 'tomato',\n",
    "     'garlic', 'pepper', 'purpl', 'onion', 'season',\n",
    "     'garbanzo', 'bean', 'feta', 'chees', 'crumbl']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pattern = r'(?u)\\b[a-zA-Z_][a-zA-Z_]+\\b'\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "\n",
    "    def mapper(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "\n",
    "    def tokenizer(self, doc):\n",
    "        return [self.mapper(t) for t in regexp_tokenize(doc, pattern=self.pattern)]\n",
    "\n",
    "    def preprocessor(self, line):\n",
    "        return ' '.join(line).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is an example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black  cheddar  chees  chili  extra  fresh  garlic  ground  \\\n",
       "0      1      1        0      0      0      1      1       1       0   \n",
       "1      0      1        1      1      1      0      0       0       1   \n",
       "\n",
       "   jalapeno  lime  oil  oliv  onion  pepper  pork  salt  tomato  virgin  \n",
       "0         0     0    1     1      0       1     0     1       1       1  \n",
       "1         1     1    1     1      1       1     1     1       0       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes = [\n",
    "    ['tomatoes', 'fresh basil', 'garlic', 'extra-virgin olive oil', 'salt', 'black pepper'],\n",
    "    ['olive oil', 'onion', 'pork', 'cheddar cheese', 'ground black pepper', 'salt', 'lime', 'jalapeno chilies'],\n",
    "]\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    preprocessor=StemmerTokenizer().preprocessor,\n",
    "    tokenizer=StemmerTokenizer().tokenizer)\n",
    "vectors = vect.fit_transform(recipes)\n",
    "\n",
    "pd.DataFrame(data=vectors.todense(), columns=sorted(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by looking at the ingredients, one could guess that the first recipe is probably italian or french - we can spot typical products, eg.: extra-vergin olive oil, fresh basil. The second one is probably mexican - jalapeno chillies is a good indicator. So, the first thing we do to recognize the cuisine is to disregard the most common ingredients such as salt, black pepper, olive oil and focus on the most unique ones. It would be of help to incorporate the information about 'commonness' into the above vectors.\n",
    "\n",
    "The answer to this is *term frequency–inverse document frequency* or [tf-idf](https://en.wikipedia.org/wiki/Tf-idf). I'm not going to go into details, it is a common approach when working with bags-of-words, we will rather focus on the outcomes. Again, we use the scikit-learn implementation [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). We use variables from the previous cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black  cheddar  chees  chili  extra  fresh  garlic  ground  \\\n",
       "0  0.342  0.244    0.000  0.000  0.000  0.342  0.342   0.342   0.000   \n",
       "1  0.000  0.219    0.308  0.308  0.308  0.000  0.000   0.000   0.308   \n",
       "\n",
       "   jalapeno   lime    oil   oliv  onion  pepper   pork   salt  tomato  virgin  \n",
       "0     0.000  0.000  0.244  0.244  0.000   0.244  0.000  0.244   0.342   0.342  \n",
       "1     0.308  0.308  0.219  0.219  0.308   0.219  0.308  0.219   0.000   0.000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "trans = TfidfTransformer()\n",
    "tfidf = trans.fit_transform(vectors)\n",
    "\n",
    "pd.DataFrame(data=tfidf.todense(), columns=sorted(vect.vocabulary_)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the rare ingredients have larger weights than the common ones - that's what we wanted! To use CountVectorizer and TfidfTransformer easier, we will chain them in a [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) (we could be using a [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), but we keep them separately so it's clearer what's going on). We'll reproduce now the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basil</th>\n",
       "      <th>black</th>\n",
       "      <th>cheddar</th>\n",
       "      <th>chees</th>\n",
       "      <th>chili</th>\n",
       "      <th>extra</th>\n",
       "      <th>fresh</th>\n",
       "      <th>garlic</th>\n",
       "      <th>ground</th>\n",
       "      <th>jalapeno</th>\n",
       "      <th>lime</th>\n",
       "      <th>oil</th>\n",
       "      <th>oliv</th>\n",
       "      <th>onion</th>\n",
       "      <th>pepper</th>\n",
       "      <th>pork</th>\n",
       "      <th>salt</th>\n",
       "      <th>tomato</th>\n",
       "      <th>virgin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.342</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basil  black  cheddar  chees  chili  extra  fresh  garlic  ground  \\\n",
       "0  0.342  0.244    0.000  0.000  0.000  0.342  0.342   0.342   0.000   \n",
       "1  0.000  0.219    0.308  0.308  0.308  0.000  0.000   0.000   0.308   \n",
       "\n",
       "   jalapeno   lime    oil   oliv  onion  pepper   pork   salt  tomato  virgin  \n",
       "0     0.000  0.000  0.244  0.244  0.000   0.244  0.000  0.244   0.342   0.342  \n",
       "1     0.308  0.308  0.219  0.219  0.308   0.219  0.308  0.219   0.000   0.000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(\n",
    "            preprocessor=StemmerTokenizer().preprocessor,\n",
    "            tokenizer=StemmerTokenizer().tokenizer)),\n",
    "        ('transformer', TfidfTransformer()),\n",
    "    ])\n",
    "\n",
    "vectors = pipeline.fit_transform(recipes)\n",
    "ingredients = sorted(pipeline.named_steps['vectorizer'].vocabulary_)\n",
    "pd.DataFrame(data=vectors.todense(), columns=ingredients).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relations between words\n",
    "\n",
    "My first classifiers were based on the above vectorization of the recipes. However, I shortly realized that listing all words without preserving the relations between them brings significant lose of information. Let's see a imple example - if we vectorize these two lists of ingredients:\n",
    "\n",
    "```\n",
    " 'red pepper, black olives'   -> ['black', 'olive', 'pepper', 'red']\n",
    " 'black pepper, green olives' -> ['black', 'olive', 'pepper', 'green']\n",
    "```\n",
    "\n",
    "The only difference now is *red* and *green*, we cannot discriminate anymore between *red pepper* and *black pepper*. This is not a minor case, there's plenty of such ambiguities.\n",
    "\n",
    "However, this is not the only argument to introduce some means of preserving the relations between words. We will see it by examining recipes with ingredients often present in certain cuisines, but not unique for them. Let's check the popularity of **dijon** and **wine** in the french cuisine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>french</th>\n",
       "      <th>not french</th>\n",
       "      <th>fraction within cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dijon</th>\n",
       "      <td>190</td>\n",
       "      <td>380</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>613</td>\n",
       "      <td>3549</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dijon and wine</th>\n",
       "      <td>87</td>\n",
       "      <td>86</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                french  not french  fraction within cuisine\n",
       "dijon              190         380                     0.33\n",
       "wine               613        3549                     0.15\n",
       "dijon and wine      87          86                     0.50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recipes_with_ingredients(ingredients, df, exact_match=False):\n",
    "\n",
    "    def number_of_common_elements(list1, list2):\n",
    "        return len(set(list1).intersection(list2))\n",
    "\n",
    "    threshold = len(ingredients) if exact_match else 1\n",
    "    return df[df.ingredients.apply(lambda row: number_of_common_elements(ingredients, row.split()) >= threshold)]\n",
    "  \n",
    "def recipes_of_cuisine(cuisine, df):\n",
    "    recipes_inside = len(df[df.cuisine == cuisine])\n",
    "    recipes_outside = len(df[df.cuisine != cuisine])\n",
    "    return recipes_inside, recipes_outside\n",
    "\n",
    "def fraction(i0, i1):\n",
    "    return float(i0) / (i0 + i1)\n",
    "\n",
    "data = pd.read_json('data/train.json')\n",
    "data['ingredients'] = data.ingredients.apply(lambda ingredients: ' '.join(ingredients))\n",
    "\n",
    "french_dijon, not_french_dijon = recipes_of_cuisine('french', recipes_with_ingredients(['dijon'], data))\n",
    "french_wine, not_french_wine = recipes_of_cuisine('french', recipes_with_ingredients(['wine'], data))\n",
    "french, not_french = recipes_of_cuisine('french', recipes_with_ingredients(['dijon', 'wine'], data, exact_match=True))\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=[\n",
    "        [french_dijon, not_french_dijon, fraction(french_dijon, not_french_dijon)],\n",
    "        [french_wine, not_french_wine, fraction(french_wine, not_french_wine)],\n",
    "        [french, not_french, fraction(french, not_french)]],\n",
    "    index=['dijon', 'wine', 'dijon and wine'],\n",
    "    columns=['french', 'not french', 'fraction within cuisine']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above table, that:\n",
    " - 33% of recipes with dijon are french\n",
    " - 15% of recipes with wine are french\n",
    "\n",
    "This is a very important information when classifying. However if we consider a simultaneous appearance of both dijon and wine in the recipe we get **50% chances the recipe is french!** This is a significant increase of our odds!\n",
    "\n",
    "**Conclusion**  \n",
    "We should use not only single words derived from a recipe, we should also use pairs of words - it allows us to preserve some valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DupleTokenizer\n",
    "\n",
    "Below I introduce *DupleTokenizer* - class inheriting from *StemmerTokenizer*, but with additional method to create pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DupleTokenizer(StemmerTokenizer):\n",
    "    \"\"\"\n",
    "    Builds upon the StemmerTokenizer: after all words in a recipe are stemmed\n",
    "    they are grouped in all possible combinations of two words and added\n",
    "    to the words' list.\n",
    "    \n",
    "    Example (simplified):\n",
    "    >>> tok = DupleTokenizer()\n",
    "    >>> tok.tokenizer(\n",
    "            tok.preprocessor([\n",
    "                'sugar', 'salt', 'black pepper'\n",
    "            ])\n",
    "        )\n",
    "    ['black', 'pepper', 'salt', 'sugar',\n",
    "     'black pepper', 'black salt', 'black sugar', 'pepper salt', 'pepper sugar', 'salt sugar']\n",
    "    \"\"\"\n",
    "    def tokenizer(self, doc):\n",
    "        \n",
    "        def duple(words):\n",
    "            duples = []\n",
    "            i = 0\n",
    "            while i < len(words)-1:\n",
    "                j = i + 1\n",
    "                while j < len(words):\n",
    "                    duples.append('%s %s' % (words[i], words[j]))\n",
    "                    j += 1\n",
    "                i += 1\n",
    "            return np.array(duples)\n",
    "        \n",
    "        words = np.array([self.mapper(t) for t in regexp_tokenize(doc, pattern=self.pattern)])\n",
    "        words = sorted(set(words))\n",
    "        words = np.hstack([words, duple(words)])\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the above tokenizer, we get some 100 times more features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StemmerTokenizer (single words):          2598\n",
      "DupleTokenizer (single words and pairs):  292246\n"
     ]
    }
   ],
   "source": [
    "def features_count(tokenizer, df):\n",
    "    vect = CountVectorizer(\n",
    "        preprocessor=tokenizer().preprocessor,\n",
    "        tokenizer=tokenizer().tokenizer)\n",
    "    return len(vect.fit(df.ingredients).vocabulary_)\n",
    "\n",
    "# It takes around 40s on my machine\n",
    "data = pd.read_json('data/train.json')\n",
    "print 'StemmerTokenizer (single words):         ', features_count(StemmerTokenizer, data)\n",
    "print 'DupleTokenizer (single words and pairs): ', features_count(DupleTokenizer, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "After extracting features we can focus on the classifier. We have around 40k training samples of 300k features, so there is much more features than observations. In such a situation we can experience curse of dimensionality and there is a high risk of overfitting. One approach to deal with this problem would be to try to reduce the number of features. However, it turns out that a direct use of a SVM with a linear kernel works very well [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). This classifier - if properly regularized by choosing a proper penalty parameter - is highly resistant to over-fitting. To deal with multiple classes the model is trained in one-vs-rest regime. It is also worth to note that linearSVC is pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "    \n",
    "tokenizerClass = DupleTokenizer\n",
    "\n",
    "# Parameters were previously found based on a 3-fold cross validation\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        preprocessor=tokenizerClass().preprocessor,\n",
    "        tokenizer=tokenizerClass().tokenizer,\n",
    "        stop_words='english',\n",
    "        max_df=1.0,\n",
    "        min_df=1,\n",
    "        binary=True,\n",
    "    )),\n",
    "    ('transformer', TfidfTransformer()),\n",
    "    ('classifier', LinearSVC(\n",
    "        C=0.78, penalty='l2', loss='squared_hinge', dual=True, max_iter=1000, random_state=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes around 50s\n",
    "train = pd.read_json('data/train.json')\n",
    "X_train = train['ingredients']\n",
    "y_train = train['cuisine']\n",
    "\n",
    "test = pd.read_json('data/test.json')\n",
    "X_test = test['ingredients']\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "prediction = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = test.copy()\n",
    "submission['cuisine'] = prediction\n",
    "submission.to_csv(\n",
    "    'submission.csv', index=False, quoting=3,\n",
    "    columns=['id', 'cuisine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the recipe to jump to the 5th place!\n",
    "\n",
    "\n",
    "![score](leaderboard.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
